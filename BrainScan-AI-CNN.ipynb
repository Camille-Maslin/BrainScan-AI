{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Brain Tumor Classification using CNN\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Author**: Camille Maslin  \n",
    "**Last Updated**: 10/01/2025\n",
    "\n",
    "### Project Description\n",
    "\n",
    "This project implements a Convolutional Neural Network (CNN) for classifying brain tumor MRI images into four categories:\n",
    "- Glioma\n",
    "- Meningioma\n",
    "- No tumor\n",
    "- Pituitary\n",
    "\n",
    "### Dataset Information\n",
    "\n",
    "- **Source**: [Brain Tumor MRI Dataset (Kaggle)](https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset)\n",
    "- **Classes**: 4 types of MRI scans\n",
    "- **Image Size**: 256x256 pixels\n",
    "\n",
    "### Technical Stack\n",
    "\n",
    "- üêç Python 3.x\n",
    "- üß† TensorFlow & Keras\n",
    "- üìä Matplotlib & Seaborn\n",
    "- üî¢ NumPy & Pandas\n",
    "- üìà Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "# Import additional libraries\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration parameters\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "EPOCHS = 20\n",
    "\n",
    "# Define data directories\n",
    "data_dir = \"Training\"\n",
    "test_data_dir = \"Testing\"\n",
    "\n",
    "# Define class names\n",
    "class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
    "n_classes = len(class_names)\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Training data path exists: {os.path.exists(data_dir)}\")\n",
    "print(f\"Testing data path exists: {os.path.exists(test_data_dir)}\")\n",
    "\n",
    "# Display versions\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tf.keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis üìä\n",
    "\n",
    "### 1.1 Dataset Overview\n",
    "\n",
    "Let's analyze our dataset structure and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_distribution(data_dir):\n",
    "    \"\"\"Analyze and visualize dataset distribution.\"\"\"\n",
    "    class_counts = {}\n",
    "    total_images = 0\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        num_images = len(os.listdir(class_path))\n",
    "        class_counts[class_name] = num_images\n",
    "        total_images += num_images\n",
    "    \n",
    "    # Plot distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
    "    plt.title('Distribution of Brain MRI Images Across Classes')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(class_counts.values()):\n",
    "        percentage = (v / total_images) * 100\n",
    "        plt.text(i, v, f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Total number of images: {total_images}\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"- {class_name}: {count} images ({(count/total_images)*100:.1f}%)\")\n",
    "\n",
    "analyze_dataset_distribution(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Image Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_characteristics(data_dir, sample_size=100):\n",
    "    \"\"\"Analyze image characteristics across classes.\"\"\"\n",
    "    characteristics = {}\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        images = os.listdir(class_path)[:sample_size]\n",
    "        \n",
    "        intensities = []\n",
    "        sizes = []\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            \n",
    "            intensities.append(np.mean(img_array))\n",
    "            sizes.append(img_array.shape)\n",
    "        \n",
    "        characteristics[class_name] = {\n",
    "            'mean_intensity': np.mean(intensities),\n",
    "            'std_intensity': np.std(intensities),\n",
    "            'sizes': sizes\n",
    "        }\n",
    "    \n",
    "    # Plot intensity distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for class_name, data in characteristics.items():\n",
    "        plt.bar(class_name, data['mean_intensity'], \n",
    "                yerr=data['std_intensity'],\n",
    "                capsize=5)\n",
    "    \n",
    "    plt.title('Mean Pixel Intensity by Class')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Mean Pixel Intensity')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return characteristics\n",
    "\n",
    "image_characteristics = analyze_image_characteristics(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sample Images Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images_by_class(data_dir, samples_per_class=3):\n",
    "    \"\"\"Display sample images from each class with their characteristics.\"\"\"\n",
    "    fig = plt.figure(figsize=(15, 4 * len(class_names)))\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        images = os.listdir(class_path)[:samples_per_class]\n",
    "        \n",
    "        for i, img_name in enumerate(images):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            \n",
    "            plt.subplot(len(class_names), samples_per_class, idx * samples_per_class + i + 1)\n",
    "            plt.imshow(img_array.astype('uint8'))\n",
    "            plt.title(f'{class_name}\\nMean: {np.mean(img_array):.1f}')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images_by_class(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(data_dir):\n",
    "    \"\"\"Assess data quality and potential issues.\"\"\"\n",
    "    quality_issues = {\n",
    "        'corrupt_images': [],\n",
    "        'size_inconsistencies': [],\n",
    "        'low_contrast': []\n",
    "    }\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            \n",
    "            try:\n",
    "                img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "                img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "                \n",
    "                # Check size consistency\n",
    "                if img_array.shape[:2] != (IMAGE_SIZE, IMAGE_SIZE):\n",
    "                    quality_issues['size_inconsistencies'].append(img_path)\n",
    "                \n",
    "                # Check contrast\n",
    "                if np.std(img_array) < 20:  # Arbitrary threshold\n",
    "                    quality_issues['low_contrast'].append(img_path)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                quality_issues['corrupt_images'].append(img_path)\n",
    "    \n",
    "    print(\"üìä Data Quality Report\")\n",
    "    print(f\"- Corrupt images found: {len(quality_issues['corrupt_images'])}\")\n",
    "    print(f\"- Size inconsistencies: {len(quality_issues['size_inconsistencies'])}\")\n",
    "    print(f\"- Low contrast images: {len(quality_issues['low_contrast'])}\")\n",
    "    \n",
    "    return quality_issues\n",
    "\n",
    "quality_report = assess_data_quality(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Advanced Data Visualization üìä\n",
    "\n",
    "Let's create more insightful visualizations to better understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_advanced_visualizations(data_dir):\n",
    "    \"\"\"Generate advanced visualizations for deeper data analysis.\"\"\"\n",
    "    \n",
    "    # 1. Pixel Intensity Distribution by Class\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for class_name in class_names:\n",
    "        intensities = []\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_path)[:100]:  # Sample 100 images\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(\n",
    "                img_path,\n",
    "                color_mode='grayscale',\n",
    "                target_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "            )\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            intensities.extend(img_array.flatten())\n",
    "        \n",
    "        sns.kdeplot(data=intensities, label=class_name, alpha=0.5)\n",
    "    \n",
    "    plt.title('Pixel Intensity Distribution by Class')\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Image Contrast Analysis\n",
    "    contrasts = {class_name: [] for class_name in class_names}\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_path)[:100]:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(\n",
    "                img_path,\n",
    "                color_mode='grayscale',\n",
    "                target_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "            )\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            contrast = np.std(img_array)\n",
    "            contrasts[class_name].append(contrast)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=pd.DataFrame(contrasts))\n",
    "    plt.title('Image Contrast Distribution by Class')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Contrast (Standard Deviation)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Spatial Feature Analysis\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        plt.subplot(1, 4, idx + 1)\n",
    "        avg_image = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n",
    "        \n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        n_images = min(100, len(os.listdir(class_path)))\n",
    "        \n",
    "        for img_name in os.listdir(class_path)[:n_images]:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(\n",
    "                img_path,\n",
    "                color_mode='grayscale',\n",
    "                target_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    "            )\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)[:,:,0]\n",
    "            avg_image += img_array\n",
    "            \n",
    "        avg_image /= n_images\n",
    "        plt.imshow(avg_image, cmap='viridis')\n",
    "        plt.title(f'Average {class_name} Pattern')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Image Size Distribution\n",
    "    sizes = {class_name: [] for class_name in class_names}\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path)  # Original size\n",
    "            sizes[class_name].append(img.size)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for class_name, class_sizes in sizes.items():\n",
    "        unique_sizes = list(set(class_sizes))\n",
    "        size_counts = [class_sizes.count(size) for size in unique_sizes]\n",
    "        plt.scatter([f\"{size[0]}x{size[1]}\" for size in unique_sizes], \n",
    "                   size_counts, \n",
    "                   label=class_name,\n",
    "                   alpha=0.6)\n",
    "    \n",
    "    plt.title('Original Image Size Distribution by Class')\n",
    "    plt.xlabel('Image Dimensions (width x height)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_advanced_visualizations(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Statistical Analysis of Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_statistics(data_dir):\n",
    "    \"\"\"Perform statistical analysis on image features.\"\"\"\n",
    "    stats = {\n",
    "        'mean_intensity': [],\n",
    "        'contrast': [],\n",
    "        'entropy': [],\n",
    "        'class': []\n",
    "    }\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        for img_name in os.listdir(class_path)[:100]:  # Sample 100 images per class\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, color_mode='grayscale')\n",
    "            img_array = tf.keras.preprocessing.image.img_to_array(img)[:,:,0]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            stats['mean_intensity'].append(np.mean(img_array))\n",
    "            stats['contrast'].append(np.std(img_array))\n",
    "            stats['entropy'].append(entropy(img_array.flatten()))\n",
    "            stats['class'].append(class_name)\n",
    "    \n",
    "    df_stats = pd.DataFrame(stats)\n",
    "    \n",
    "    # Create pairplot\n",
    "    sns.pairplot(df_stats, hue='class', diag_kind='kde')\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    numeric_cols = ['mean_intensity', 'contrast', 'entropy']\n",
    "    sns.heatmap(df_stats[numeric_cols].corr(), \n",
    "                annot=True, \n",
    "                cmap='coolwarm',\n",
    "                center=0)\n",
    "    plt.title('Correlation between Image Features')\n",
    "    plt.show()\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "image_stats = analyze_image_statistics(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Configuration üìä\n",
    "\n",
    "Let's set up our data pipeline and configuration parameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, batch_size, image_size):\n",
    "    \"\"\"Load and preprocess the image dataset.\"\"\"\n",
    "    return tf.keras.utils.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        seed=123,\n",
    "        shuffle=True,\n",
    "        image_size=(image_size, image_size),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    dataset = load_data(data_dir, BATCH_SIZE, IMAGE_SIZE)\n",
    "    test_dataset = load_data(test_data_dir, BATCH_SIZE, IMAGE_SIZE)\n",
    "    print(\"‚úÖ Data loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Visualization üîç\n",
    "\n",
    "Let's prepare our data and visualize some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(ds, train_split=0.9, val_split=0.1):\n",
    "    \"\"\"Split dataset into training and validation sets.\"\"\"\n",
    "    ds_size = len(ds)\n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "# Split and optimize dataset\n",
    "train_ds, val_ds = split_dataset(dataset)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Visualize sample images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture üß†\n",
    "\n",
    "Define our CNN model architecture for brain tumor classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"Create and compile the CNN model.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Rescaling(1./255, input_shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS)),\n",
    "        layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training üöÄ\n",
    "\n",
    "Train our CNN model and monitor its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation üìä\n",
    "\n",
    "Let's perform a comprehensive evaluation of our model using multiple metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(y_true, y_pred_proba):\n",
    "    \"\"\"Plot ROC curves for each class.\"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    y_test_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def create_normalized_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Create and plot normalized confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Normalized Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Original confusion matrix\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix (Raw Counts)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model_performance(model, dataset):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    y_pred_proba = []\n",
    "    \n",
    "    for images, labels in dataset:\n",
    "        predictions = model.predict(images)\n",
    "        y_pred.extend(np.argmax(predictions, axis=1))\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred_proba.extend(predictions)\n",
    "    \n",
    "    y_pred_proba = np.array(y_pred_proba)\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plot_roc_curves(y_true, y_pred_proba)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    create_normalized_confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nüìä Detailed Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluate_model_performance(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Predictions üîç\n",
    "\n",
    "Visualize model predictions on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(model, dataset, num_images=9):\n",
    "    \"\"\"Plot sample predictions with confidence scores.\"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for images, labels in dataset.take(1):\n",
    "        predictions = model.predict(images)\n",
    "        \n",
    "        for i in range(min(num_images, len(images))):\n",
    "            plt.subplot(3, 3, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            \n",
    "            predicted_class = class_names[np.argmax(predictions[i])]\n",
    "            true_class = class_names[labels[i]]\n",
    "            confidence = 100 * np.max(predictions[i])\n",
    "            \n",
    "            color = 'green' if predicted_class == true_class else 'red'\n",
    "            plt.title(f'True: {true_class}\\nPred: {predicted_class}\\nConf: {confidence:.1f}%',\n",
    "                      color=color)\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot sample predictions\n",
    "plot_predictions(model, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion and Insights üéØ\n",
    "\n",
    "### Data Analysis Summary\n",
    "- Distribution of classes\n",
    "- Image quality metrics\n",
    "- Key characteristics by tumor type\n",
    "\n",
    "### Model Performance\n",
    "- Test Accuracy: {test_accuracy:.2%}\n",
    "- Per-class performance analysis\n",
    "- Error analysis and insights\n",
    "\n",
    "### Key Findings\n",
    "1. Class distribution patterns\n",
    "2. Distinctive features by tumor type\n",
    "3. Model strengths and limitations\n",
    "\n",
    "### Future Research Directions\n",
    "1. Advanced preprocessing techniques\n",
    "2. Feature importance analysis\n",
    "3. Model interpretability studies\n",
    "4. Cross-validation with external datasets\n",
    "\n",
    "### Thank you! üôè\n",
    "For questions or improvements, please contact the author."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
